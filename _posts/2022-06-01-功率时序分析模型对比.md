---
layout: post
title: "功率时序分析模型对比"
date: 2022-06-01 18:14:00 +0800
categories: AI
tags: ["机器学习"]
comments: true
---

风电场的功率输出本身就是一条“情绪多变”的时间序列：它跟天气、机组状态、电网友好程度都有关系。
对于电网调度和电站运营来说，**提前 1–2 天尽量看清这条曲线的走向**，意味着可以更从容地安排备用容量、检修计划以及与电网侧的协商空间，少一点弃风和临时救火。

在这个前提下，我们选用了 Bangaluru_Wind Generation 数据集，基于 2017–2021 年的历史数据，搭建了一组从简单到相对复杂的序列模型（LSTM、GRU、Conv1D、Encoder–Decoder 等），用统一的窗口长度和训练配置，去预测未来 48 小时的发电功率。

这件事有两层意义：

1. **对业务侧**来说：
   我们能直观看到，不同建模思路在“48 小时风电预测”这个具体任务上的表现差异，哪些结构在当前数据条件下更稳、哪些更容易翻车，为后续真正上线的预测服务提供一个比较可靠的参考起点，而不是“拍脑袋选模型”。

2. **对技术侧**来说：
   这组实验把多种常见的时间序列深度学习模型放在同一赛道上，用同一套评估指标对比，让人更容易形成一种“模型直觉”：什么情况下简单 LSTM 已经够用，什么时候多变量、多层结构才真正带来收益，哪些“看上去很高级”的结构在现实数据里其实并不划算。

简单讲，这不是为了“哪一个模型一定最好”，而是借着一个真实的风电场景，把模型选择这件事从抽象讨论，拉回到具体的误差数字和可视化曲线里。

💡 本节使用 **Bangaluru_Wind Generation** 数据集，数据整体设置如下：

1. 时间范围：2017–2021 年，其中 **2017–2020 年** 作为训练集，**2021 年 1–6 月** 作为验证集。
2. Sliding Window 长度为 **48 个时间步**，预测未来 **48 小时**的功率。
3. 训练轮数（epochs）统一设为 **40**，方便模型之间横向对比。


## 简单 LSTM

> **模型简介：**
> 这是最基础的单层 LSTM 回归模型，只输入历史功率序列，直接预测未来 48 小时的功率。结构非常“朴素”：一层 128 单元的 LSTM，用来提取时间依赖，再接三层逐步收缩的全连接层（64→32→1）完成回归输出。参数量只有 ~7.7 万，在计算开销和表达能力之间做了一个折中，可以看作整个实验中的 **基线模型（baseline）**。

```text
Model: "sequential_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 lstm_1 (LSTM)               (None, 128)               66560

 dense_3 (Dense)             (None, 64)                8256

 dense_4 (Dense)             (None, 32)                2080

 dense_5 (Dense)             (None, 1)                 33

=================================================================
Total params: 76,929
Trainable params: 76,929
Non-trainable params: 0
_________________________________________________________________
```

（图表略）

mean absolute error：`257.01451520284013`

root mean squared error： `337.215700750134`

> 从评估指标看，简单 LSTM 在所有模型中已经给出了 **相对不错的基线误差水平**，说明即便是单层结构，也能捕捉到一部分风电功率的时序模式。

---

## 归一化之后的简单 LSTM

> **模型简介：**
> 在这个版本中，我们在数据预处理阶段加入了归一化，并使用了 **双向 LSTM（Bidirectional LSTM）**。直观理解，就是让模型同时“向前看”和“向后看”历史窗口中的信息，从而更细致地捕捉序列模式。结构同样是 LSTM 编码 + 全连接回归头，但参数量增加到了 ~15 万，相比简单 LSTM 更“厚实”一些。

```text
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 bidirectional (Bidirectiona  (None, 256)              133120
 l)

 dense (Dense)               (None, 64)                16448

 dense_1 (Dense)             (None, 32)                2080

 dense_2 (Dense)             (None, 1)                 33

=================================================================
Total params: 151,681
Trainable params: 151,681
Non-trainable params: 0
_________________________________________________________________
```

（图表略）

mean absolute error：`311.4765823842088`

root mean squared error： `388.79620825606236`

> 有意思的是：**归一化 + 双向 LSTM 并没有带来更优的误差**，反而略逊于最基础的 LSTM。这提示我们：
> 1）归一化的方式、范围可能需要更细致地调整；
> 2）双向结构在这种单向时间预测任务中不一定天然占优，甚至可能因为过拟合或训练不稳定带来波动。

---

## 多层简单 LSTM

> **模型简介：**
> 这个模型在“简单 LSTM”的基础上叠加了一层 LSTM，形成了典型的 **Stacked LSTM** 结构：前一层 LSTM 输出全序列特征（return_sequences=True），后一层再进一步压缩成 128 维的高阶时间表示。下游依旧是 64→32→1 的三层 Dense 回归头。总参数量提升到 ~47 万，可学习的模式明显更多。

```text
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 lstm (LSTM)                 (None, 48, 256)           264192

 lstm_1 (LSTM)               (None, 128)               197120

 dense (Dense)               (None, 64)                8256

 dense_1 (Dense)             (None, 32)                2080

 dense_2 (Dense)             (None, 1)                 33

=================================================================
Total params: 471,681
Trainable params: 471,681
Non-trainable params: 0
_________________________________________________________________
```

（图表略）

mean absolute error：`272.04515764872235`

root mean squared error： `345.20799681686344`

> 从结果看，多层 LSTM 在 MAE/RMSE 上 **轻微优于简单 LSTM**，但提升有限。这往往意味着：当前数据集规模和噪声水平下，继续堆叠层数带来的收益已经开始变得边际递减，模型复杂度上去了，但“能学到的额外东西”并不多。

---

## 双向简单 LSTM

> **模型简介：**
> 这一部分使用的是 **双向 LSTM（Bidirectional LSTM）+ 全连接层** 的组合，和“归一化之后的简单 LSTM”类似，但这里可以看作是在 “不额外强调预处理细节” 的前提下，单纯比较双向结构对性能的影响。双向 LSTM 会在同一时间窗口内同时编码“从过去到现在”和“从现在回看过去”的信息。

（结构图表略）

mean absolute error：`277.8488557179769`

root mean squared error： `351.04406347793855`

> 从结果来看，双向 LSTM 的表现 **介于简单 LSTM 与多层 LSTM 之间**：说明对于风电功率这种带有一定周期性的序列，双向建模能捕捉到更多局部模式，但在“未来预测”这个任务设定下，它并不是决定性优势。

---

## Conv1D

> **模型简介：**
> Conv1D 模型尝试用 **一维卷积神经网络** 来替代 RNN 结构：
>
> - 前两层 Conv1D 相当于在时间轴上滑动窗口做特征提取，可以理解为“自动学习特征工程”；
> - Flatten 之后接多层 Dense，用于整合特征并输出最终功率预测。
>   相较 LSTM，Conv1D 更擅长捕捉 **局部时间模式和短期依赖**，但对长序列依赖的表达能力稍弱。

```text
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 conv1d (Conv1D)             (None, 127, 256)          768

 conv1d_1 (Conv1D)           (None, 126, 128)          65664

 flatten (Flatten)           (None, 16128)             0

 dense (Dense)               (None, 64)                1032256

 dense_1 (Dense)             (None, 32)                2080

 dense_2 (Dense)             (None, 1)                 33

=================================================================
Total params: 1,100,801
Trainable params: 1,100,801
Non-trainable params: 0
_________________________________________________________________
```

（图表略）

mean absolute error：`359.53889788309726`

root mean squared error： `423.94092680841146`

> 这个模型的参数量已经超过 110 万，但误差却明显高于 LSTM 系列，说明：
>
> - 纯卷积在这个数据集上对“中长期依赖”的建模能力不足；
> - 目前的结构（如卷积核大小、层数、池化策略等）还没有针对风电功率的特性做精细调优。
>   换句话说，它更像是一个“对比用”的尝试，而不是最终选型。

---

## Simple GRU

> **模型简介：**
> GRU 可以视作 LSTM 的一个“精简版亲戚”，通过合并部分门控结构，在保持时序建模能力的同时减少参数数量、加快训练。这里的 Simple GRU 只有一层 128 单元的 GRU，加上三层 Dense 回归头，整体结构和 Simple LSTM 如出一辙，但参数量更少（~6 万）。

```text
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 gru (GRU)                   (None, 128)               50304

 dense (Dense)               (None, 64)                8256

 dense_1 (Dense)             (None, 32)                2080

 dense_2 (Dense)             (None, 1)                 33

=================================================================
Total params: 60,673
Trainable params: 60,673
Non-trainable params: 0
_________________________________________________________________
```

（图表略）

mean absolute error：`515.1815938568116`

root mean squared error： `619.6165305534781`

> 从表现来看，这个 GRU 配置 **明显不如 LSTM 系列**：
>
> - 可能是当前超参数（学习率、正则化、初始化等）更适配 LSTM；
> - 也可能说明在这个任务上，GRU 的“轻量化”反而限制了对复杂非线性关系的刻画。
>   它提醒我们：结构更简单并不自动意味着“更好更稳”。

---

## Multivariate LSTM

> **模型简介：**
> 前面的模型大多只使用单一功率时间序列，这里开始引入 **多变量（Multivariate）**：在输入端加入额外特征，例如风速、风向、温度等（具体特征视预处理而定）。模型本身仍是一层 128 单元的 LSTM + Dense 回归头，但输入维度更高，能够从 **多个物理量的联合变化** 中学习到模式。

```text
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 lstm (LSTM)                 (None, 128)               71680

 dense (Dense)               (None, 64)                8256

 dense_1 (Dense)             (None, 32)                2080

 dense_2 (Dense)             (None, 1)                 33

=================================================================
Total params: 82,049
Trainable params: 82,049
Non-trainable params: 0
```

（图表略）

mean absolute error：`314.79579213460283`

root mean squared error：`387.96425438431964`

> 多变量 LSTM 的表现与前面的单变量模型相比 **有一定竞争力但不算突出**，说明虽然增加外部特征有助于模型理解场景，但：
>
> - 特征工程质量（例如归一化方式、滞后项设计）会直接影响收益；
> - 简单堆特征，不一定就能换来显著提升。

---

## **Stacked** Multivariate LSTM

> **模型简介：**
> 在多变量输入的基础上，我们进一步叠加 LSTM 层，形成 **多变量 + 多层（Stacked）结构**：
>
> - 第一层 LSTM 输出整个序列的隐状态；
> - 第二层再进行更高阶的时间特征抽象；
> - 最终通过 Dense 回归头输出功率预测。
>   这种组合理论上更适合捕捉复杂的交互关系和长期依赖。

```text
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 lstm (LSTM)                 (None, 48, 256)           274432

 lstm_1 (LSTM)               (None, 128)               197120

 dense (Dense)               (None, 64)                8256

 dense_1 (Dense)             (None, 32)                2080

 dense_2 (Dense)             (None, 1)                 33

=================================================================
Total params: 481,921
Trainable params: 481,921
Non-trainable params: 0
_________________________________________________________________
```

（图表略）

mean absolute error：`352.4635163879395`

root mean squared error： `435.35770194226257`

> 但从结果看，**复杂度的提升并没有换来更好的误差**，反而略有退步：
>
> - 一种可能是过拟合（尤其是在特征较多、样本量有限的情况下）；
> - 也可能是当前超参数设置更适合相对简单的网络。
>   它提醒我们：多层、多变量并不是“越多越好”，而是需要和数据规模、噪声水平一起权衡。

---

## Bidirectional Multivariate LSTM

> **模型简介：**
> 这个模型在多变量输入的基础上，进一步采用 **双向 LSTM**，让模型在一个窗口内同时从“前向”和“后向”两个方向去理解特征的变化模式。对于具有周期性、局部模式明显的时间序列，这种结构有机会捕捉到更加细致的变化细节。

```text
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 bidirectional (Bidirectiona  (None, 256)              143360
 l)

 dense (Dense)               (None, 64)                16448

 dense_1 (Dense)             (None, 32)                2080

 dense_2 (Dense)             (None, 1)                 33

=================================================================
Total params: 161,921
Trainable params: 161,921
Non-trainable params: 0
_________________________________________________________________
```

（图表略）

mean absolute error：`304.0681587219238`

root mean squared error： `364.8291245847453`

> 从 MAE / RMSE 看，Bidirectional Multivariate LSTM 的表现 **相对均衡且略有优势**，可以视作当前实验配置下的一个“表现较稳的折中方案”：
>
> - 参数量适中；
> - 既利用了多变量特征，又在窗口内引入双向信息；
> - 泛化能力相对前几种更有竞争力。

---

## Encoder - Decoder LSTM

> **模型简介：**
> Encoder–Decoder 结构是时间序列预测中的一个经典方案：
>
> - **Encoder** 部分通过多层 LSTM 将输入序列压缩成一个“上下文向量”；
> - 通过 RepeatVector 将这个向量扩展到目标序列长度；
> - **Decoder** 再使用 LSTM 逐步生成输出序列，最后由 TimeDistributed 层在每个时间步上输出功率预测。
>   直观一点理解：前半段负责“把过去读懂”，后半段负责“按理解写出未来”。

```text
Model: "sequential_3"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 lstm_6 (LSTM)               (None, 48, 128)           66560

 lstm_7 (LSTM)               (None, 32)                20608

 repeat_vector_1 (RepeatVect  (None, 64, 32)           0
 or)

 lstm_8 (LSTM)               (None, 64, 32)            8320

 lstm_9 (LSTM)               (None, 64, 128)           82432

 time_distributed_1 (TimeDis  (None, 64, 1)            129
 tributed)

=================================================================
Total params: 178,049
Trainable params: 178,049
Non-trainable params: 0
_________________________________________________________________
```

（图表略）

mean absolute error：`314.97556800842284`

root mean squared error： `377.519952340769`

> 从结果来看，Encoder–Decoder 的误差 **大致处于中游偏上**：
>
> - 优点是结构上适合多步预测，能自然生成整段未来轨迹；
> - 不足之处是训练更敏感，对学习率、正则、序列长度等参数更挑剔。
>   在后续工作中，如果引入注意力机制（Attention）或更精细的调参，它还有一定提升空间。
