---
layout: post
title: "机器学习分类问题中的评价指标体系"
date: 2021-09-19 18:14:00 +0800
categories: AI
tags: ["机器学习"]
comments: true
---

在处理分类任务时（如垃圾邮件识别、欺诈检测、疾病诊断等），**模型效果好不好** 绝不能只看“对了多少次”。
不同类型的错误（把坏样本判成好样本 vs 把好样本判成坏样本）在业务上往往代价不同，因此需要一整套评价指标来刻画模型的表现。

本文围绕以下几个核心指标展开：

- 准确率（Accuracy）
- 精确率（Precision）
- 召回率（Recall）
- P-R 曲线与平均精度（Average Precision, AP）
- F 指标 / F1 指标
- ROC 曲线与相关概念（TPR、FPR）


## 混淆矩阵与基本概念

在二分类任务中，我们通常把真实标签分为“正类”（Positive）和“负类”（Negative），模型的预测结果也分为“预测为正”和“预测为负”。由此得到四个基本计数：

- **TP（True Positive）**：真实为正，预测为正
- **FP（False Positive）**：真实为负，预测为正
- **TN（True Negative）**：真实为负，预测为负
- **FN（False Negative）**：真实为正，预测为负

> 图 1：二分类问题的混淆矩阵示意
> `![图1 混淆矩阵示意](images/confusion-matrix-placeholder.png)  <!-- TODO: 替换为实际混淆矩阵图片 -->`

在此基础上，我们可以定义各种评价指标。

---

## 准确率（Accuracy）

**准确率**描述的是：模型预测对的样本占全部样本的比例：

$$
\text{Accuracy} = \frac{TP + TN}{TP + FP + TN + FN}
$$

直观上，Accuracy 越高，模型整体越“准”。
但在**类别极度不平衡**的场景下（例如正样本仅占 1%），仅靠准确率容易产生误导：一个“全部预测为负类”的模型也可能有 99% 的准确率，却完全没用。

因此，在很多实际任务中，我们还需要关注**精确率**和**召回率**。

---

## 精确率（Precision）与召回率（Recall）

- 精确率：

  > 在被识别为正类别的样本中，确实为正类别的比例是多少？

- 召回率：

  > 在所有正类别样本中，被正确识别为正类别的比例是多少？

形式化定义如下：

$$
\text{Precision} = \frac{TP}{TP + FP}
$$

$$
\text{Recall} = \frac{TP}{TP + FN}
$$

理解方式：

- **Precision 高**：说明“判成正类”的样本，大多是真的正类，**误报少**；
- **Recall 高**：说明真实正类大部分都被找出来了，**漏报少**。

在很多应用里，两者是此消彼长的：
提高阈值可以减小误报（Precision 上升），但可能漏掉更多真正的正例（Recall 下降）；
降低阈值可以多抓一些正例（Recall 上升），但也会把更多负例错判为正例（Precision 下降）。

---

## P-R 曲线与平均精度（AP）

### P-R 曲线（Precision-Recall Curve）

给定一个**可调阈值**的分类器（例如输出的是“属于正类”的概率），我们可以：

1. 设定一系列不同的阈值（如 0.1, 0.2, ..., 0.9）；
2. 对每个阈值计算对应的 Precision 和 Recall；
3. 以 Recall 为横轴、Precision 为纵轴，画出曲线，这就是 **P-R 曲线**。

> 图 2：P-R 曲线示例
> `![图2 P-R 曲线示意图](images/pr-curve-placeholder.png)  <!-- TODO: 替换为实际 P-R 曲线图片 -->`

示意图展示了一个从左到右整体略有下降的曲线：Recall 越高，Precision 一般会渐渐下降。

### 平均精度（Average Precision, AP）

**Average Precision（AP）** 可以理解为 P-R 曲线下的面积（Area under P-R curve）：

- 曲线越“往右上角贴近”，AP 越高；
- AP 介于 0 和 1 之间，数值越大，整体性能越好。

在目标检测、信息检索等任务中，AP 是非常常用的综合指标，可以用来比较不同模型或不同超参数设置的优劣。

---

## 指标与 F1 指标

在实际业务中，我们常常需要一个**同时考虑 Precision 和 Recall 的单一指标**，方便排序与比较。这就是 **F 指标（F-Measure）** 的用处。

### 通用形式：$ F\_\beta $

通用公式为：

$$
F_\beta = (1 + \beta^2) \cdot \frac{\text{Precision} \cdot \text{Recall}}{\beta^2 \cdot \text{Precision} + \text{Recall}}
$$

其中：

- $\beta$ 是一个权重参数；
- $\beta > 1$ 时，更强调 **Recall**；
- $\beta < 1$ 时，更强调 **Precision**。

> 图 3：F 指标公式示意
> `![图3 F-beta 指标公式示意](images/f-beta-placeholder.png)  <!-- TODO: 替换为实际 F-Measure 公式图片 -->`

### F1 指标（最常用）

当 $\beta = 1$ 时，Precision 和 Recall 具有同等重要性，这时的 F 指标通常写作 **F1**：

$$
F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
$$

$F1$ 可以看作 Precision 和 Recall 的**调和平均数**：

- 只有当两者都较高时，F1 才会高；
- 若其中一个很低，F1 会被显著拉低。

因此，在需要“兼顾误报和漏报”的场景中，F1 是一个非常实用的综合度量。

---

## ROC 曲线与相关指标

### TPR 与 FPR

PDF 中的 ROC 部分给出了两条提示：

- “在所有正类别样本中，被正确识别为正类别的比例是多少？”
- “在所有负类别样本中，被正确识别为正类别的比例是多少？”

这里分别对应：

- **TPR（True Positive Rate）**，也叫 **召回率 Recall**：

$$
\text{TPR} = \frac{TP}{TP + FN}
$$

- **FPR（False Positive Rate）**，即“把负类错分为正类”的比例：

$$
\text{FPR} = \frac{FP}{FP + TN}
$$

有时也会顺带提到：

- **TNR（True Negative Rate）**：真负例率
- $\text{FPR} = 1 - \text{TNR}$

上述关系在 PDF 底部也以公式形式出现。

### ROC 曲线（Receiver Operating Characteristic）

和 P-R 曲线类似，ROC 曲线也是通过**扫描不同阈值**得到的：

1. 对每个阈值，计算一对 $\text{FPR}, \text{TPR}$；
2. 以 FPR 作为横轴、TPR 作为纵轴；
3. 将这些点连接成曲线，即为 **ROC 曲线**。

> 图 4：ROC 曲线示例
> `![图4 ROC 曲线示意图](images/roc-curve-placeholder.png)  <!-- TODO: 替换为实际 ROC 曲线图片 -->`

图中展示了一条橙色的 ROC 曲线，以及一条虚线的基线（“No Skill”），用于对比一个随机分类器与一个有能力的模型之间的差异。

一般来说：

- 曲线越“凸向左上角”，模型区分正负类的能力越好；
- 随机模型大致沿对角线分布（TPR ≈ FPR）。

### AUC（Area Under ROC Curve）【可选扩展】

实践中我们经常会计算 ROC 曲线下的面积，即 **AUC（Area Under Curve）**：

- $ \text{AUC} \in [0, 1] $；
- AUC 越大，模型整体性能越好；
- AUC = 0.5 对应“瞎猜”（随机分类器）。

---

## P-R 与 ROC：如何选择指标？

在实际问题中，如何在 Accuracy、Precision、Recall、F1、AP、ROC/AUC 之间做选择，通常取决于：

1. **类别是否极度不平衡**

   - 极不平衡问题（如罕见疾病、欺诈检测）：
     - Accuracy 意义有限，更关注 Recall / Precision / F1 / P-R 曲线、AP。

   - 类别比较均衡：
     - Accuracy 和 ROC/AUC 也有很高参考价值。

2. **业务更在乎“误报”还是“漏报”**

   - 更害怕漏掉正例（如癌症筛查）：更关注 **Recall**，甚至采用 $F\_\beta) 中 (\beta > 1$。
   - 更害怕误报（如某些告警系统）：更关注 **Precision**，可采用 $\beta < 1$。

3. **是否需要比较不同阈值下的整体表现**

   - 关注“随着阈值变化模型表现如何”：看 **P-R 曲线 / AP** 或 **ROC / AUC**；
   - 只在某一个固定阈值下运行：看对应的 Precision、Recall、F1 即可。
